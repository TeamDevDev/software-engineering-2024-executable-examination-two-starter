{"amount_correct": 16, "percentage_score": 32, "report_time": "2024-11-07 18:16:11", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for README.md", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 5 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype the every word in the Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this executable examination.", "count": 3, "exact": true}, "status": true, "path": "../README.md"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 10, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting exactly 10"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 11 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Ensure that question_two.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_two.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_two.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 3 fragment(s) in the question_two.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_two.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 8, "exact": false}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 6 comment(s) in the question_two.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_two.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_two.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_two.py"}, {"description": "Ensure that question_three.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_three.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_three.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_three.py", "diagnostic": "Found 2 fragment(s) in the question_three.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_three.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 8, "exact": false}, "status": true, "path": "questions/question_three.py"}, {"description": "Create a sufficient number of single-line comments in question_three.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_three.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_three.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f1302eff0e0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_difference - AssertionError: Failed on case with identical \n     coverage reports\n     \n     test_question_one.py::test_compute_coverage_difference\n       - Status: Failed\n         Line: 29\n         Exact: [] == [CoverageItem...covered=True)] ...\n         Message: Failed on case with identical coverage reports\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_difference\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_one.py\n       Line number: 29\n       Message: AssertionError: Failed on case with identical coverage reports\n     assert [] == [CoverageItem...covered=True)]\n       \n       Right contains 3 more items, first extra item: CoverageItem(id=1, line='line1', covered=True)\n       \n       Full diff:\n       + []\n       - [\n       -     CoverageItem(id=1, line='line1', covered=True),\n       -     CoverageItem(id=2, line='line2', covered=True),\n       -     CoverageItem(id=3, line='line3', covered=True),\n       - ]\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 17\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_difference():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         item1 = CoverageItem(1, \"line1\", True)\n         item2 = CoverageItem(2, \"line2\", True)\n         item3 = CoverageItem(3, \"line3\", True)\n         item4 = CoverageItem(4, \"line4\", True)\n         item5 = CoverageItem(5, \"line5\", True)\n         item6 = CoverageItem(6, \"line6\", True)\n         item7 = CoverageItem(1, \"line1\", False)\n         item8 = CoverageItem(2, \"line2\", False)\n         item9 = CoverageItem(3, \"line3\", False)\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item1, item2, item3]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item4, item5, item6])\n             == []\n         ), \"Failed on case with no common coverage\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [\n             item2,\n             item3,\n         ], \"Failed on case with partial overlap\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item3, item2, item1]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_intersection([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item7, item8, item9])\n             == []\n         ), \"Failed on case with same ids but not covered\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item1, item2, item3]) == []\n         ), \"Failed on case with identical coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item4, item5, item6]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with no common coverage\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [item1], \"Failed on case with partial overlap\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item3, item2, item1]) == []\n         ), \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_difference([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item7, item8, item9]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with same ids but different coverage status\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fc1ca19ec60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_score - AssertionError: Failed on all items covered\n     \n     test_question_one.py::test_compute_coverage_score\n       - Status: Passed\n         Line: 92\n         Code: calculate_coverage_score([]) == 0.0\n         Exact: 0.0 == 0.0 ...\n       - Status: Failed\n         Line: 95\n         Exact: 0.0 == 1.0 ...\n         Message: Failed on all items covered\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_score\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_one.py\n       Line number: 95\n       Message: AssertionError: Failed on all items covered\n     assert 0.0 == 1.0\n      +  where 0.0 = calculate_coverage_score([CoverageItem(id=1, line='line1', covered=True), CoverageItem(id=1, \n     line='line1', covered=True), CoverageItem(id=1, line='line1', covered=True), CoverageItem(id=1, line='line1', \n     covered=True), CoverageItem(id=1, line='line1', covered=True)])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 88\n     @pytest.mark.question_one_part_b\n     def test_compute_coverage_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # test with an empty list\n         assert calculate_coverage_score([]) == 0.0, \"Failed on empty list\"\n         # test with all items covered\n         all_covered = [CoverageItem(1, \"line1\", True) for _ in range(5)]\n         assert calculate_coverage_score(all_covered) == 1.0, \"Failed on all items covered\"\n         # test with no items covered\n         none_covered = [CoverageItem(1, \"line1\", False) for _ in range(5)]\n         assert calculate_coverage_score(none_covered) == 0.0, \"Failed on no items covered\"\n         # test with some items covered\n         some_covered = [\n             CoverageItem(1, \"line1\", True),\n             CoverageItem(2, \"line2\", False),\n             CoverageItem(3, \"line3\", True),\n         ]\n         assert (\n             calculate_coverage_score(some_covered) == 2 / 3\n         ), \"Failed on some items covered\"\n         # test with one item covered\n         one_covered = [CoverageItem(1, \"line1\", True)]\n         assert calculate_coverage_score(one_covered) == 1.0, \"Failed on one item covered\"\n         # test with one item not covered\n         one_not_covered = [CoverageItem(1, \"line1\", False)]\n         assert (\n             calculate_coverage_score(one_not_covered) == 0.0\n         ), \"Failed on one item not covered\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7feab258ec60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score - AssertionError: assert 0.0 == 0.5\n     \n     test_question_one.py::test_compute_mutation_score\n       - Status: Passed\n         Line: 127\n         Code: compute_mutation_score([]) == 0.0\n         Exact: 0.0 == 0.0 ...\n       - Status: Passed\n         Line: 129\n         Code: (\n             compute_mutation_score([Mutant(1, \"line1\", False), Mutant(2, \"line2\", False)])\n             == 0.0\n         )\n         Exact: 0.0 == 0.0 ...\n       - Status: Failed\n         Line: 134\n         Exact: 0.0 == 0.5 ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_one.py\n       Line number: 134\n       Message: AssertionError: assert 0.0 == 0.5\n      +  where 0.0 = compute_mutation_score([Mutant(id=1, line='line1', detected=True, equivalent=False), Mutant(id=2, \n     line='line2', detected=False, equivalent=False)])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 118\n     @pytest.mark.question_one_part_c\n     def test_compute_mutation_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # summary of the checks:\n         # check 1: Empty list of mutants\n         # check 2: Empty\n         # check 3: Partially detected\n         # check 4: Fully detected\n         # check 1: Empty list of mutants\n         assert compute_mutation_score([]) == 0.0\n         # check 2: All undetected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", False), Mutant(2, \"line2\", False)])\n             == 0.0\n         )\n         # check 3: Partially detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", False)])\n             == 0.5\n         )\n         # check 4: All detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", True)])\n             == 1.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fe76f606ff0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score_equivalent_aware - AssertionError: assert 0.0 == 0.5\n     \n     test_question_one.py::test_compute_mutation_score_equivalent_aware\n       - Status: Passed\n         Line: 149\n         Code: compute_mutation_score_equivalent_aware([]) == 0.0\n         Exact: 0.0 == 0.0 ...\n       - Status: Passed\n         Line: 151\n         Code: (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", False, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.0\n         )\n         Exact: 0.0 == 0.0 ...\n       - Status: Failed\n         Line: 158\n         Exact: 0.0 == 0.5 ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score_equivalent_aware\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_one.py\n       Line number: 158\n       Message: AssertionError: assert 0.0 == 0.5\n      +  where 0.0 = compute_mutation_score_equivalent_aware([Mutant(id=1, line='line1', detected=True, equivalent=False), \n     Mutant(id=2, line='line2', detected=False, equivalent=False)])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 145\n     @pytest.mark.question_one_part_d\n     def test_compute_mutation_score_equivalent_aware():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # check 1: empty list of mutants\n         assert compute_mutation_score_equivalent_aware([]) == 0.0\n         # check 2: all undetected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", False, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.0\n         )\n         # check 3: partially detected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.5\n         )\n         # check 4: all detected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, False), Mutant(2, \"line2\", True, False)]\n             )\n             == 1.0\n         )\n         # check 5: equivalent mutants should be ignored\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, True), Mutant(2, \"line2\", False, True)]\n             )\n             == 0.0\n         )\n         # check 6: mix of detected, undetected, and equivalent mutants\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [\n                     Mutant(1, \"line1\", True, False),\n                     Mutant(2, \"line2\", False, False),\n                     Mutant(3, \"line3\", True, True),\n                     Mutant(4, \"line4\", False, True),\n                 ]\n             )\n             == 0.5\n         )\n         # check 7: all of the mutants are equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [\n                     Mutant(1, \"line1\", True, True),\n                     Mutant(2, \"line2\", False, True),\n                     Mutant(3, \"line3\", True, True),\n                     Mutant(4, \"line4\", False, True),\n                 ]\n             )\n             == 0.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f79d653f350>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code - AssertionError: Failed to count the number of \n     print functions in the source code\n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code_embedded_print - AssertionError: Failed to count\n     the number of print functions in the source code with a print string\n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code_check_violation - AssertionError: Failed to \n     count the number of print functions in the source code\n     \n     test_question_two.py::test_extract_prints_from_source_code\n       - Status: Failed\n         Line: 71\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code\n     \n     test_question_two.py::test_extract_prints_from_source_code_embedded_print\n       - Status: Failed\n         Line: 80\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code with a print string\n     \n     test_question_two.py::test_zero_prints_in_source_code\n       - Status: Passed\n         Line: 89\n         Code: (\n             count == 0\n         )\n         Exact: 0 == 0\n     \n     test_question_two.py::test_zero_prints_in_source_code_embedded\n       - Status: Passed\n         Line: 98\n         Code: (\n             count == 0\n         )\n         Exact: 0 == 0\n     \n     test_question_two.py::test_extract_prints_from_source_code_check_violation\n       - Status: Failed\n         Line: 107\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_extract_prints_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 71\n       Message: AssertionError: Failed to count the number of print functions in the source code\n     assert 0 == 3\n       Name: tests/test_question_two.py::test_extract_prints_from_source_code_embedded_print\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 80\n       Message: AssertionError: Failed to count the number of print functions in the source code with a print string\n     assert 0 == 3\n       Name: tests/test_question_two.py::test_extract_prints_from_source_code_check_violation\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 107\n       Message: AssertionError: Failed to count the number of print functions in the source code\n     assert 0 == 3\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 67\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 76\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code_embedded_print():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints_embedded_print)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code with a print string\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 103\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code_check_violation():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code\"\n         not_violation = detect_print_function_violation_linter(\n             source_code_one_three_prints, 2\n         )\n         assert not_violation, \"Failed to detect the violation of the print function limit\"\n         expected_violation = detect_print_function_violation_linter(\n             source_code_one_three_prints\n         )\n         assert (\n             expected_violation\n         ), \"Failed to detect the violation of the default print function limit\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f9de555f110>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_extract_plus_operator_from_source_code - AssertionError: Failed to count the \n     number of plus operators in the source code (exist)\n     FAILED tests/test_question_two.py::test_extract_minus_operator_from_source_code - AssertionError: Failed to count the \n     number of minus operators in the source code (exist)\n     \n     test_question_two.py::test_extract_plus_operator_from_source_code\n       - Status: Failed\n         Line: 126\n         Exact: 0 == 1\n         Message: Failed to count the number of plus operators in the source code (exist)\n     \n     test_question_two.py::test_extract_minus_operator_from_source_code\n       - Status: Failed\n         Line: 139\n         Exact: 0 == 1\n         Message: Failed to count the number of minus operators in the source code (exist)\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_extract_plus_operator_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 126\n       Message: AssertionError: Failed to count the number of plus operators in the source code (exist)\n     assert 0 == 1\n       Name: tests/test_question_two.py::test_extract_minus_operator_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 139\n       Message: AssertionError: Failed to count the number of minus operators in the source code (exist)\n     assert 0 == 1\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 122\n     @pytest.mark.question_two_part_b\n     def test_extract_plus_operator_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_plus_operator_in_assignments(source_code_two_one_plus)\n         assert (\n             count == 1\n         ), \"Failed to count the number of plus operators in the source code (exist)\"\n         count = count_plus_operator_in_assignments(source_code_two_one_minus)\n         assert (\n             count == 0\n         ), \"Failed to count the number of plus operators in the source code (not exist)\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 135\n     @pytest.mark.question_two_part_b\n     def test_extract_minus_operator_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_minus_operator_in_assignments(source_code_two_one_minus)\n         assert (\n             count == 1\n         ), \"Failed to count the number of minus operators in the source code (exist)\"\n         count = count_minus_operator_in_assignments(source_code_two_one_plus)\n         assert (\n             count == 0\n         ), \"Failed to count the number of minus operators in the source code (not exist)\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fbf095571a0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_check_print_violations_with_list - AssertionError: Failed to check the \n     violations of print functions in the list\n     \n     test_question_two.py::test_check_print_violations_with_list\n       - Status: Failed\n         Line: 168\n         Exact: {} == {'\\ndef examp... 5\"\\n': False} ...\n         Message: Failed to check the violations of print functions in the list\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_check_print_violations_with_list\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 168\n       Message: AssertionError: Failed to check the violations of print functions in the list\n     assert {} == {'\\ndef examp... 5\"\\n': False}\n       \n       Right contains 4 more items:\n       {'\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"print x is greater than 5\")\\n': True,\n        '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"x is greater than 5\")\\n': True,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': False,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than \n     5\"\\n': False}\n       \n       Full diff:\n       + {}\n       - {\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"print x is greater than 5\")\\n': True,\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"x is greater than 5\")\\n': True,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': False,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than\n     5\"\\n': False,\n       - }\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 148\n     @pytest.mark.question_two_part_c\n     def test_check_print_violations_with_list():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert (\n             violations == expected_violations\n         ), \"Failed to check the violations of print functions in the list\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f3d71056c60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_check_overall_violations - assert {} == {'\\ndef examp... 5\"\\n': False}\n     \n     test_question_two.py::test_check_overall_violations\n       - Status: Failed\n         Line: 193\n         Exact: {} == {'\\ndef examp... 5\"\\n': False} ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_check_overall_violations\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_two.py\n       Line number: 193\n       Message: assert {} == {'\\ndef examp... 5\"\\n': False}\n       \n       Right contains 4 more items:\n       {'\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"print x is greater than 5\")\\n': True,\n        '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"x is greater than 5\")\\n': True,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': False,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than \n     5\"\\n': False}\n       \n       Full diff:\n       + {}\n       - {\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"print x is greater than 5\")\\n': True,\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"x is greater than 5\")\\n': True,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': False,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than\n     5\"\\n': False,\n       - }\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 173\n     @pytest.mark.question_two_part_d\n     def test_check_overall_violations():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert violations == expected_violations\n         # check the violations of plus operators in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         overall_violations = has_overall_print_violation_linter(violations)\n         # assert the results\n         assert (\n             overall_violations\n         ), \"Failed to check the overall violations of print functions in the list\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fc779e43260>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_mutate_functions_building_blocks - AssertionError: delete_random_character \n     failed: length mismatch\n     FAILED tests/test_question_three.py::test_overall_mutation_function - IndexError: Cannot choose from an empty sequence\n     \n     test_question_three.py::test_mutate_functions_building_blocks\n       - Status: Failed\n         Line: 25\n         Exact: 0 == (5 - 1) ...\n         Message: delete_random_character failed: length mismatch\n     \n     test_question_three.py::test_overall_mutation_function\n       - Status: Failed\n         Line: 347\n         Exact: IndexError\n         Message: Cannot choose from an empty sequence\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_mutate_functions_building_blocks\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_three.py\n       Line number: 25\n       Message: AssertionError: delete_random_character failed: length mismatch\n     assert 0 == (5 - 1)\n      +  where 0 = len('')\n      +  and   5 = len('hello')\n       Name: tests/test_question_three.py::test_overall_mutation_function\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_three.py\n       Line number: 347\n       Message: IndexError: Cannot choose from an empty sequence\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 19\n     @pytest.mark.question_three_part_a\n     def test_mutate_functions_building_blocks():\n         \"\"\"Test for a question part.\"\"\"\n         # test delete_random_character function\n         s = \"hello\"\n         result = delete_random_character(s)\n         assert len(result) == len(s) - 1, \"delete_random_character failed: length mismatch\"\n         assert all(\n             c in s for c in result\n         ), \"delete_random_character failed: unexpected characters\"\n         assert (\n             delete_random_character(\"\") == \"\"\n         ), \"delete_random_character failed: empty string case\"\n         # test insert_random_character function\n         s = \"world\"\n         result = insert_random_character(s)\n         assert len(result) == len(s) + 1, \"insert_random_character failed: length mismatch\"\n         assert all(\n             c in string.printable for c in result\n         ), \"insert_random_character failed: non-printable character inserted\"\n         # test flip_random_character function\n         s = \"test\"\n         result = flip_random_character(s)\n         assert len(result) == len(s), \"flip_random_character failed: length mismatch\"\n         assert any(\n             c != s[i] for i, c in enumerate(result)\n         ), \"flip_random_character failed: no character flipped\"\n         assert (\n             flip_random_character(\"\") == \"\"\n         ), \"flip_random_character failed: empty string case\"\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 51\n     @pytest.mark.question_three_part_a\n     def test_overall_mutation_function():\n         \"\"\"Test for a question part.\"\"\"\n         # test mutate function with a non-empty string\n         s = \"example\"\n         result = mutate(s)\n         assert len(result) in [\n             len(s) - 1,\n             len(s),\n             len(s) + 1,\n         ], \"mutate failed: length invariant violated\"\n         assert all(\n             c in string.printable for c in result\n         ), \"mutate failed: non-printable character in result\"\n         # test mutate function with an empty string\n         s = \"\"\n         result = mutate(s)\n         assert (\n             result == \"\" or len(result) == 1\n         ), \"mutate failed: empty string case invariant violated\"\n         assert all(\n             c in string.printable for c in result\n         ), \"mutate failed: non-printable character in result for empty string\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f27cc67b230>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_equals_for_strings - AssertionError: equals failed: different strings of same \n     length should not be equal\n     FAILED tests/test_question_three.py::test_equals_for_strings_after_mutation - IndexError: Cannot choose from an empty \n     sequence\n     \n     test_question_three.py::test_equals_for_strings\n       - Status: Passed\n         Line: 80\n         Code: equals_for_mutation(\n             \"hello\", \"hello\"\n         )\n         Exact: True ...\n       - Status: Failed\n         Line: 84\n         Exact: not True ...\n         Message: equals failed: different strings of same length should not be equal\n     \n     test_question_three.py::test_equals_for_strings_after_mutation\n       - Status: Failed\n         Line: 347\n         Exact: IndexError\n         Message: Cannot choose from an empty sequence\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_equals_for_strings\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_three.py\n       Line number: 84\n       Message: AssertionError: equals failed: different strings of same length should not be equal\n     assert not True\n      +  where True = equals_for_mutation('hello', 'world')\n       Name: tests/test_question_three.py::test_equals_for_strings_after_mutation\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_three.py\n       Line number: 347\n       Message: IndexError: Cannot choose from an empty sequence\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 76\n     @pytest.mark.question_three_part_b\n     def test_equals_for_strings():\n         \"\"\"Test for a question part.\"\"\"\n         # test case: equal strings\n         assert equals_for_mutation(\n             \"hello\", \"hello\"\n         ), \"equals failed: identical strings should be equal\"\n         # test case: different strings of same length\n         assert not equals_for_mutation(\n             \"hello\", \"world\"\n         ), \"equals failed: different strings of same length should not be equal\"\n         # test case: different strings of different lengths\n         assert not equals_for_mutation(\n             \"hello\", \"helloo\"\n         ), \"equals failed: strings of different lengths should not be equal\"\n         # test case: empty strings\n         assert equals_for_mutation(\"\", \"\"), \"equals failed: empty strings should be equal\"\n         # test case: one empty string and one non-empty string\n         assert not equals_for_mutation(\n             \"\", \"nonempty\"\n         ), \"equals failed: empty string and non-empty string should not be equal\"\n         # test case: case sensitivity\n         assert not equals_for_mutation(\n             \"Hello\", \"hello\"\n         ), \"equals failed: strings with different cases should not be equal\"\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 103\n     @pytest.mark.question_three_part_b\n     def test_equals_for_strings_after_mutation():\n         \"\"\"Test for a question part.\"\"\"\n         # test case: equal strings\n         assert not equals_for_mutation(\n             \"hello\", mutate(\"hello\")\n         ), \"equals failed: identical strings should be equal\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fd4d127b2f0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_check_multiple_string_equality - KeyError: ''\n     \n     test_question_three.py::test_check_multiple_string_equality\n       - Status: Passed\n         Line: 119\n         Code: all(\n             result.values()\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 127\n         Code: not any(\n             result.values()\n         )\n         Exact: not False ...\n       - Status: Passed\n         Line: 134\n         Code: all(\n             result.values()\n         )\n         Exact: True ...\n       - Status: Failed\n         Line: 141\n         Exact: KeyError\n         Message: ''\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_check_multiple_string_equality\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_three.py\n       Line number: 141\n       Message: KeyError: ''\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 112\n     @pytest.mark.question_three_part_c\n     def test_check_multiple_string_equality():\n         \"\"\"Test for a question part.\"\"\"\n         # test case: all strings equal to comparison string\n         input_strings = [\"test\", \"test\", \"test\"]\n         comparison_string = \"test\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert all(\n             result.values()\n         ), \"check_multiple_string_equality failed: all strings should be equal to comparison string\"\n     \n         # test case: no strings equal to comparison string\n         input_strings = [\"test1\", \"test2\", \"test3\"]\n         comparison_string = \"exemplify\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert not any(\n             result.values()\n         ), \"check_multiple_string_equality failed: no strings should be equal to comparison string\"\n         # test case: empty strings\n         input_strings = [\"\", \"\"]\n         comparison_string = \"\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert all(\n             result.values()\n         ), \"check_multiple_string_equality failed: empty strings should be equal to empty comparison string\"\n         # test case: one empty string and one non-empty string\n         input_strings = [\"\", \"nonempty\"]\n         comparison_string = \"\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert result[\n             \"\"\n         ], \"check_multiple_string_equality failed: empty string should be equal to empty comparison string\"\n         assert not result[\n             \"nonempty\"\n         ], \"check_multiple_string_equality failed: non-empty string should not be equal to empty comparison string\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f0f2a1d6f90>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_generate_fuzzer_values - AssertionError: Character is not in range\n     \n     test_question_three.py::test_generate_fuzzer_values\n       - Status: Passed\n         Line: 154\n         Code: len(result) <= max_length\n         Exact: 1 <= 10 ...\n       - Status: Failed\n         Line: 159\n         Exact: 65 <= 32 ...\n         Message: Character is not in range\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_generate_fuzzer_values\n       Path: <...>/software-engineering-2024-executable-examination-two-starter/exam/tests/test_question_three.py\n       Line number: 159\n       Message: AssertionError: Character is not in range\n     assert 65 <= 32\n      +  where 32 = ord(' ')\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 149\n     @pytest.mark.question_three_part_d\n     def test_generate_fuzzer_values():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         max_length = 10\n         result = generate_fuzzer_values(max_length)\n         assert len(result) <= max_length, \"Generated string is too long\"\n         char_start = 65\n         char_range = 26\n         result = generate_fuzzer_values(100, char_start, char_range)\n         for char in result:\n             assert (\n                 char_start <= ord(char) < char_start + char_range\n             ), \"Character is not in range\"\n         result = generate_fuzzer_values(0)\n         assert result == \"\", \"Empty string not generated\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:177:5: D103 Missing docstring in public function\n         |\n     177 | def compute_mutation_score(mutants):\n         |     ^^^^^^^^^^^^^^^^^^^^^^ D103\n     178 |     mutation_score = 0.0\n     179 |     return mutation_score\n         |\n     \n     questions/question_one.py:214:5: D103 Missing docstring in public function\n         |\n     214 | def compute_mutation_score_equivalent_aware(mutants):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     215 |     mutation_score = 0.0\n     216 |     return mutation_score\n         |\n     \n     Found 2 errors."}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "status": false, "diagnostic": "Would reformat: questions/question_one.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:85: error: Need type annotation for \"coverage_intersection\" (hint: \"coverage_intersection: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:94: error: Need type annotation for \"coverage_difference\" (hint: \"coverage_difference: list[<type>] = ...\")  [var-annotated]\n     Found 2 errors in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 5, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 7, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_two.py", "status": false, "diagnostic": "questions/question_two.py:78:5: F841 Local variable `tree` is assigned to but never used\n        |\n     76 |     \"\"\"Count the number of print function calls in the given Python source code.\"\"\"\n     77 |     # extract the abstract syntax tree (AST) from the source code\n     78 |     tree = ast.parse(source_code)\n        |     ^^^^ F841\n     79 |     print_count = 0\n     80 |     # return the number of print function calls\n        |\n        = help: Remove assignment to unused variable `tree`\n     \n     questions/question_two.py:151:5: F841 Local variable `tree` is assigned to but never used\n         |\n     149 |     \"\"\"Count the number of times a specific operator is used in assignment statements.\"\"\"\n     150 |     # TODO: count the number of specific operator types inside of the provided source code\n     151 |     tree = ast.parse(source_code)\n         |     ^^^^ F841\n     152 |     operator_count = 0\n     153 |     return operator_count\n         |\n         = help: Remove assignment to unused variable `tree`\n     \n     questions/question_two.py:162:5: D103 Missing docstring in public function\n         |\n     162 | def count_minus_operator_in_assignments(source_code):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     163 |     # TODO: call the count_operator_in_assignments function with the appropriate operator type\n     164 |     return 0\n         |\n     \n     questions/question_two.py:244:5: D103 Missing docstring in public function\n         |\n     244 | def has_overall_print_violation_linter(violations_dict):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     245 |     return False\n         |\n     \n     Found 4 errors.\n     No fixes available (2 hidden fixes can be enabled with the `--unsafe-fixes` option)."}, {"description": "Ensure that Question 2 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_two.py --check", "status": false, "diagnostic": "Would reformat: questions/question_two.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 2 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_two.py", "status": false, "diagnostic": "questions/question_two.py:207: error: Need type annotation for \"violations_dict\" (hint: \"violations_dict: dict[<type>, <type>] = ...\")  [var-annotated]\n     Found 1 error in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 2 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_two.py --count", "fragment": 7, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_two.py --count", "fragment": 7, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_two.py --count", "fragment": 0, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_three.py", "status": false, "diagnostic": "questions/question_three.py:9:20: F401 [*] `typing.Dict` imported but unused\n        |\n      8 | import random\n      9 | from typing import Dict, List\n        |                    ^^^^ F401\n     10 | \n     11 | from questions import constants\n        |\n        = help: Remove unused import\n     \n     questions/question_three.py:9:26: F401 [*] `typing.List` imported but unused\n        |\n      8 | import random\n      9 | from typing import Dict, List\n        |                          ^^^^ F401\n     10 | \n     11 | from questions import constants\n        |\n        = help: Remove unused import\n     \n     questions/question_three.py:100:5: F841 Local variable `mutator` is assigned to but never used\n         |\n      98 |     # three mutation functions that are available\n      99 |     mutators = []\n     100 |     mutator = random.choice(mutators)\n         |     ^^^^^^^ F841\n     101 |     return \"\"\n         |\n         = help: Remove assignment to unused variable `mutator`\n     \n     Found 3 errors.\n     [*] 2 fixable with the `--fix` option (1 hidden fix can be enabled with the `--unsafe-fixes` option)."}, {"description": "Ensure that Question 3 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_three.py --check", "status": true}, {"description": "Ensure that Question 3 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_three.py", "status": false, "diagnostic": "questions/question_three.py:99: error: Need type annotation for \"mutators\" (hint: \"mutators: list[<type>] = ...\")  [var-annotated]\n     Found 1 error in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 3 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_three.py --count", "fragment": 7, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_three.py --count", "fragment": 7, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 3 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_three.py --count", "fragment": 0, "count": 1, "exact": true}, "status": true}]}